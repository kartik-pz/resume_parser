{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f98f984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from evaluate import evaluator\n",
    "from openvino import Core\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "from optimum.intel import OVConfig, OVModelForTokenClassification, OVQuantizationConfig, OVQuantizer\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d322912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"manishiitg/resume-ner\"\n",
    "DATASET_NAME = \"\"\n",
    "\n",
    "base_model_path = Path(f\"models/{MODEL_ID}\")\n",
    "fp32_model_path = base_model_path.with_name(base_model_path.name + \"_FP32\")\n",
    "int8_ptq_model_path = base_model_path.with_name(base_model_path.name + \"_INT8_PTQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854beb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OVModelForTokenClassification.from_pretrained(MODEL_ID, export=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# See how the tokenizer for the given model converts input text to model input values\n",
    "print(tokenizer(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb34fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Sachinkelenjaguri/Resume_dataset\", revision= \"main\", data_files= \"UpdatedResumeDataSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d83521bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(examples, tokenizer):\n",
    "    \"\"\"convert the text from the dataset into tokens in the format that the model expects\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"Resume\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length= 512,\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24591fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['Category', 'Resume']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ds\n",
    "ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_examples = dataset[\"train\"].filter(lambda x: x[\"Category\"] == \"Data Science\")\n",
    "train_dataset = filtered_examples.map(lambda x: preprocess_fn(x, tokenizer), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Quantize the model\n",
    "quantizer = OVQuantizer.from_pretrained(model)\n",
    "ov_config = OVConfig(quantization_config=OVQuantizationConfig())\n",
    "quantizer.quantize(calibration_dataset=train_dataset, ov_config=ov_config, save_directory=int8_ptq_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfc358",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_ptq = OVModelForTokenClassification.from_pretrained(int8_ptq_model_path)\n",
    "ov_qa_pipeline_ptq = pipeline(\"token-classification\", model=quantized_model_ptq, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a85f0233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'DATE', 'score': np.float32(0.9824913), 'index': 3, 'word': 'may', 'start': 18, 'end': 21}\n",
      "{'entity': 'DATE', 'score': np.float32(0.98756987), 'index': 4, 'word': '2013', 'start': 22, 'end': 26}\n",
      "{'entity': 'DATE', 'score': np.float32(0.54650015), 'index': 5, 'word': 'to', 'start': 27, 'end': 29}\n",
      "{'entity': 'DATE', 'score': np.float32(0.9854757), 'index': 6, 'word': 'may', 'start': 30, 'end': 33}\n",
      "{'entity': 'DATE', 'score': np.float32(0.98744106), 'index': 7, 'word': '2017', 'start': 34, 'end': 38}\n",
      "{'entity': 'EducationDegree', 'score': np.float32(0.9468082), 'index': 8, 'word': 'b', 'start': 39, 'end': 40}\n",
      "{'entity': 'ORG', 'score': np.float32(0.5314321), 'index': 11, 'word': 'ui', 'start': 43, 'end': 45}\n",
      "{'entity': 'ORG', 'score': np.float32(0.6193985), 'index': 12, 'word': '##t', 'start': 45, 'end': 46}\n",
      "{'entity': 'Designation', 'score': np.float32(0.74931425), 'index': 17, 'word': 'data', 'start': 52, 'end': 56}\n",
      "{'entity': 'Designation', 'score': np.float32(0.7668792), 'index': 18, 'word': 'scientist', 'start': 57, 'end': 66}\n",
      "{'entity': 'Designation', 'score': np.float32(0.80389476), 'index': 19, 'word': 'data', 'start': 67, 'end': 71}\n",
      "{'entity': 'Designation', 'score': np.float32(0.6762103), 'index': 20, 'word': 'scientist', 'start': 72, 'end': 81}\n",
      "{'entity': 'ExperianceYears', 'score': np.float32(0.87187886), 'index': 35, 'word': '1', 'start': 137, 'end': 138}\n",
      "{'entity': 'ExperianceYears', 'score': np.float32(0.93645674), 'index': 36, 'word': 'year', 'start': 139, 'end': 143}\n",
      "{'entity': 'ExperianceYears', 'score': np.float32(0.8004057), 'index': 37, 'word': 'months', 'start': 144, 'end': 150}\n"
     ]
    }
   ],
   "source": [
    "result = ov_qa_pipeline_ptq(\"Education Details May 2013 to May 2017 B.E UIT-RGPV Data Scientist Data Scientist - Matelabs Skill Details Python- Exprience - Less than 1 year months Statsmodels- Exprience\")\n",
    "for item in result:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ca0d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_resume_entities(entity_list):\n",
    "    \"\"\"\n",
    "    Converts a flat list of entities from the HF pipeline into a structured dictionary.\n",
    "    It also attempts to merge consecutive tokens of the same entity type.\n",
    "    \"\"\"\n",
    "    if not entity_list:\n",
    "        return {}\n",
    "\n",
    "    # Sort by start index to process in order, pipeline output is usually sorted by appearance\n",
    "    # but explicit sort by 'index' or 'start' is safer if there's any doubt.\n",
    "    # The pipeline output you showed is already implicitly in order.\n",
    "    # entity_list.sort(key=lambda x: x['start']) # Or x['index']\n",
    "\n",
    "    structured_output = {}\n",
    "    merged_entities = []\n",
    "    current_entity_text = \"\"\n",
    "    current_entity_label = None\n",
    "    current_entity_score_sum = 0\n",
    "    current_entity_token_count = 0\n",
    "    last_end_index = -1\n",
    "\n",
    "    for entity in entity_list:\n",
    "        word = entity['word']\n",
    "        label = entity['entity']\n",
    "        score = float(entity['score']) # Ensure it's a Python float\n",
    "        start_index = entity['start']\n",
    "\n",
    "        # Remove \"##\" from subword tokens for cleaner text,\n",
    "        # though pipeline with aggregation usually handles this.\n",
    "        if word.startswith(\"##\"):\n",
    "            word = word[2:]\n",
    "            # For direct concatenation, we might not want a space if it's a subword\n",
    "            # However, the pipeline's 'word' for aggregated entities should be clean.\n",
    "            # If 'word' can be a subword token, a more robust re-tokenization or\n",
    "            # space-joining logic might be needed based on original text.\n",
    "            # For now, assume 'word' is a complete word or subword that can be joined with space.\n",
    "\n",
    "        if current_entity_label == label and (start_index == last_end_index or start_index == last_end_index + 1 or word.startswith(\"##\") or not current_entity_text):\n",
    "            # Continue current entity if same label and tokens are adjacent or overlapping\n",
    "            # or if it's a subword token.\n",
    "            # The `word.startswith(\"##\")` is a simple heuristic for subwords.\n",
    "            # A more robust check would be if the current `start_index`\n",
    "            # immediately follows `last_end_index` without spaces IF original text was available.\n",
    "            if current_entity_text and not word.startswith(\"##\") and not current_entity_text.endswith(\"-\"): # Add space if not a subword and not ending with hyphen\n",
    "                current_entity_text += \" \"\n",
    "            current_entity_text += word\n",
    "            current_entity_score_sum += score\n",
    "            current_entity_token_count += 1\n",
    "        else:\n",
    "            # New entity or different type, finalize previous one\n",
    "            if current_entity_label and current_entity_text:\n",
    "                avg_score = current_entity_score_sum / current_entity_token_count if current_entity_token_count > 0 else 0\n",
    "                merged_entities.append({\n",
    "                    \"text\": current_entity_text.strip(),\n",
    "                    \"label\": current_entity_label,\n",
    "                    \"score\": round(avg_score, 4)\n",
    "                })\n",
    "\n",
    "            # Start new entity\n",
    "            current_entity_text = word\n",
    "            current_entity_label = label\n",
    "            current_entity_score_sum = score\n",
    "            current_entity_token_count = 1\n",
    "\n",
    "        last_end_index = entity['end']\n",
    "\n",
    "    # Add the last processed entity\n",
    "    if current_entity_label and current_entity_text:\n",
    "        avg_score = current_entity_score_sum / current_entity_token_count if current_entity_token_count > 0 else 0\n",
    "        merged_entities.append({\n",
    "            \"text\": current_entity_text.strip(),\n",
    "            \"label\": current_entity_label,\n",
    "            \"score\": round(avg_score, 4)\n",
    "        })\n",
    "\n",
    "    # Populate the structured_output dictionary\n",
    "    for item in merged_entities:\n",
    "        label = item['label']\n",
    "        text = item['text']\n",
    "        # score = item['score'] # Optionally include score if needed in final JSON\n",
    "\n",
    "        # Basic deduplication within each label category\n",
    "        if label not in structured_output:\n",
    "            structured_output[label] = []\n",
    "        if text not in structured_output[label]:\n",
    "            structured_output[label].append(text)\n",
    "            \n",
    "    # --- Attempt to create more complex structures (OPTIONAL & HEURISTIC) ---\n",
    "    # This part is more complex and error-prone without more context or rules.\n",
    "    # For now, let's keep the primary output flat as derived from merged_entities.\n",
    "    # We can add specific post-processing for 'DATE' and 'ExperianceYears'\n",
    "    \n",
    "    # Post-process DATE for better readability\n",
    "    if \"DATE\" in structured_output:\n",
    "        # This is a very simple join. A more robust date parser would be better.\n",
    "        # Example: \"May\", \"2013\", \"to\", \"May\", \"2017\" -> \"May 2013 to May 2017\"\n",
    "        # This simple logic assumes dates appear contiguously in the `merged_entities`\n",
    "        # which may not always be true if other entities interleave.\n",
    "        # A better approach would be to process `merged_entities` directly for date ranges.\n",
    "        \n",
    "        # For now, if multiple date parts are separate, we just list them.\n",
    "        # True date range combination requires looking at original text and proximity.\n",
    "        pass # The current merging logic already groups adjacent dates\n",
    "\n",
    "    # Post-process ExperianceYears\n",
    "    if \"ExperianceYears\" in structured_output:\n",
    "        # Try to form more meaningful phrases like \"Less than 1 year\"\n",
    "        # This is highly heuristic.\n",
    "        # The current merging logic groups them if they are consecutive.\n",
    "        pass\n",
    "\n",
    "    return structured_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06a8446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"DATE\": [\n",
      "    \"may 2013 to may 2017\"\n",
      "  ],\n",
      "  \"EducationDegree\": [\n",
      "    \"b\"\n",
      "  ],\n",
      "  \"ORG\": [\n",
      "    \"ui t\"\n",
      "  ],\n",
      "  \"Designation\": [\n",
      "    \"data scientist data scientist\"\n",
      "  ],\n",
      "  \"ExperianceYears\": [\n",
      "    \"1 year months\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "structured_json_output = structure_resume_entities(result)\n",
    "\n",
    "import json\n",
    "print(json.dumps(structured_json_output, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
